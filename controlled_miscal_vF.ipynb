{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac94b3cd-183c-4d94-9062-6cb2aac42021",
   "metadata": {},
   "source": [
    "## Base Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea82cb-20bc-44ec-9bfa-dd4a9e947658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "from collections import Counter\n",
    "from scipy.stats import pareto\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "random.seed(1217)\n",
    "F_dataset = pd.read_json('path', orient='records')\n",
    "##length is 417000\n",
    "def create_powerlaw_p(F_dataset, pareto_alpha):\n",
    "    #create p\n",
    "    all_facts = F_dataset['fact'].tolist()\n",
    "    new_facts = []\n",
    "    for fact in all_facts:\n",
    "        reps = pareto.rvs(b=pareto_alpha, scale=1)\n",
    "        reps = int(np.ceil(reps))\n",
    "        new_facts.extend([fact] * reps)\n",
    "    return new_facts\n",
    "\n",
    "def create_uniform_p(F_dataset):\n",
    "    #create p, which is already uniform\n",
    "    new_facts = F_dataset['fact'].tolist()\n",
    "    return new_facts\n",
    "\n",
    "def sample(new_facts, size):\n",
    "    #sample with replacement\n",
    "    training_data = random.sample(new_facts, k=size)\n",
    "    return training_data\n",
    "\n",
    "def mono_calc(new_facts):\n",
    "    fact_counts = Counter(new_facts)\n",
    "    num_mono = sum(1 for count in fact_counts.values() if count == 1)\n",
    "    mono_pct = num_mono / len(new_facts)\n",
    "    return mono_pct\n",
    "    \n",
    "class MovieFactMarkovChain:\n",
    "    def __init__(self, order):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(lambda: defaultdict(float))\n",
    "        self.initial_counts = defaultdict(float)\n",
    "        self.starts = []\n",
    "        self.EOS = '<EOS>'\n",
    "        \n",
    "    def tokenize_fact(self, fact):\n",
    "        # Modify this method if city facts have a specific format\n",
    "        return fact.strip().split()\n",
    "\n",
    "    def calculate_log_probability(self, fact):\n",
    "        MIN_PROB = 1e-7  # Increased minimum probability\n",
    "        \n",
    "        tokens = self.tokenize_fact(fact) + [self.EOS]\n",
    "        if len(tokens) < self.order:\n",
    "            return np.log(MIN_PROB)\n",
    "        \n",
    "        # Start with initial probability\n",
    "        start_tokens = tuple(tokens[:self.order])\n",
    "        log_probability = np.log(max(self.initial_probs.get(start_tokens, MIN_PROB), MIN_PROB))\n",
    "        \n",
    "        # Use order-n transitions\n",
    "        for i in range(len(tokens) - self.order):\n",
    "            current = tuple(tokens[i:i + self.order])\n",
    "            next_token = tokens[i + self.order]\n",
    "            \n",
    "            # Get transition probability with minimum threshold\n",
    "            trans_prob = max(self.transitions.get(current, {}).get(next_token, MIN_PROB), MIN_PROB)\n",
    "            log_probability += np.log(trans_prob)\n",
    "        \n",
    "        return log_probability\n",
    "    \n",
    "    def train(self, facts):\n",
    "        num_facts = len(facts)\n",
    "        start_time = time.time()\n",
    "\n",
    "        for fact in facts:\n",
    "            tokens = self.tokenize_fact(fact) + [self.EOS]\n",
    "            if len(tokens) < self.order + 1:\n",
    "                continue  # Skip if not enough tokens\n",
    "            \n",
    "            start_tokens = tuple(tokens[:self.order])\n",
    "            self.starts.append(start_tokens)\n",
    "            self.initial_counts[start_tokens] += 1\n",
    "            \n",
    "            for i in range(len(tokens) - self.order):\n",
    "                current = tuple(tokens[i:i + self.order])\n",
    "                next_token = tokens[i + self.order]\n",
    "                self.transitions[current][next_token] += 1\n",
    "        \n",
    "        total_starts = sum(self.initial_counts.values())\n",
    "        self.initial_probs = {start: count / total_starts for start, count in self.initial_counts.items()}\n",
    "        \n",
    "        for current, next_tokens in self.transitions.items():\n",
    "            total = sum(next_tokens.values())\n",
    "            for next_token in next_tokens:\n",
    "                next_tokens[next_token] /= total\n",
    "                    \n",
    "        training_time = time.time() - start_time\n",
    "    \n",
    "    def generate_facts(self, num_generations):\n",
    "        \"\"\"Generate new facts\"\"\"\n",
    "        generated_facts = []\n",
    "        # print(f\"\\nGenerating {num_generations:,} facts...\")\n",
    "        \n",
    "        # with tqdm(total=num_generations) as pbar:\n",
    "        while len(generated_facts) < num_generations:\n",
    "            # Pick a random start sequence\n",
    "            current = random.choices(\n",
    "                population=list(self.initial_probs.keys()),\n",
    "                weights=list(self.initial_probs.values()),\n",
    "                k=1\n",
    "            )[0]\n",
    "            result = list(current)\n",
    "            \n",
    "            # Generate until EOS token is produced\n",
    "            while True:\n",
    "                if current not in self.transitions:\n",
    "                    break  # Can't continue from here\n",
    "                \n",
    "                # Get possible next tokens and their probabilities\n",
    "                possible_tokens = list(self.transitions[current].keys())\n",
    "                probabilities = list(self.transitions[current].values())\n",
    "                \n",
    "                # Generate next token\n",
    "                next_token = np.random.choice(possible_tokens, p=probabilities)\n",
    "                if next_token == self.EOS:\n",
    "                    break  # End of sequence\n",
    "                result.append(next_token)\n",
    "                \n",
    "                # Update current context\n",
    "                current = tuple(result[-self.order:])\n",
    "            \n",
    "            # Add generated fact\n",
    "            generated_fact = ' '.join(result)\n",
    "            generated_facts.append({\n",
    "                'fact': generated_fact,\n",
    "                'generated': 1\n",
    "            })\n",
    "            # pbar.update(1)\n",
    "        \n",
    "        # Create DataFrame and save\n",
    "        generated_df = pd.DataFrame(generated_facts)            \n",
    "        return generated_df\n",
    "\n",
    "#-----------------------------------------check hallucinations and calibration-----------------------------------------------\n",
    "def check_hallucinations(generated_data, training_data, F_dataset):\n",
    "    \"\"\"Analyze hallucination rates in generated data\n",
    "    \n",
    "    Hallucinations are generated facts that don't exist in F (false statements)\n",
    "    Not just facts missing from training data O\n",
    "    \"\"\"\n",
    "    # print(\"\\nChecking hallucinations...\")\n",
    "    \n",
    "    generated_facts = set(generated_data['fact'])\n",
    "    F_facts = set(F_dataset)  # All true facts\n",
    "    \n",
    "    # Hallucinations are generated facts not in F\n",
    "    hallucinations = list(generated_facts - F_facts)\n",
    "    hallucination_rate = len(hallucinations) / len(generated_data)\n",
    "    \n",
    "    # Also calculate \"unseen but true\" rate, not found this to be low most time\n",
    "    training_facts = set(training_data)\n",
    "    unseen_true = list(generated_facts - training_facts - set(hallucinations))\n",
    "    unseen_true_rate = len(unseen_true) / len(generated_data)\n",
    "    \n",
    "    return {\n",
    "        'hallucination_rate': hallucination_rate,\n",
    "        'hallucinated_facts': hallucinations,\n",
    "        'unseen_true_rate': unseen_true_rate,\n",
    "        'unseen_true_facts': unseen_true\n",
    "    }\n",
    "\n",
    "def create_epsilon_induced_bins(epsilon: float):\n",
    "   \"\"\"\n",
    "   Creates bins according to Definition 7 in the paper (page 169).\n",
    "   \n",
    "   Args:\n",
    "       epsilon: Controls the bin width (between 0 and 1)\n",
    "       \n",
    "   Returns:\n",
    "       List of tuples representing (bin_start, bin_end) in probability space\n",
    "   \"\"\"\n",
    "   if epsilon < 0:\n",
    "       raise ValueError(\"Epsilon must be non-negative\")\n",
    "   \n",
    "   if epsilon == 0:\n",
    "       # For epsilon = 0, we create a bin for each unique probability in g\n",
    "       # This will be handled in the main calibration function\n",
    "       return \"finest\"\n",
    "   \n",
    "   if epsilon >= 1:\n",
    "       # When epsilon = 1, return single bin for all probabilities\n",
    "       return [(0, 1)]\n",
    "   \n",
    "   bins = []\n",
    "   i = 0\n",
    "   \n",
    "   while True:\n",
    "       upper = (1-epsilon)**i\n",
    "       lower = (1-epsilon)**(i+1)\n",
    "       \n",
    "       # If lower bound gets very small, make it 0 and make this our last bin\n",
    "       if lower < 1e-10:\n",
    "           bins.append((0, upper))\n",
    "           break\n",
    "           \n",
    "       if upper - lower > 1e-10:\n",
    "           bins.append((lower, upper))\n",
    "       \n",
    "       i += 1\n",
    "   ##append final bin for edge case where everything is 1\n",
    "   bins.append((1.0, 1.0))\n",
    "   return bins\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from bisect import bisect_right\n",
    "\n",
    "def create_adaptive_bins(g_probs: list, b: int) -> list:\n",
    "    \"\"\"\n",
    "    Creates adaptive bins according to Definition 2 in Kalai's paper.\n",
    "    Optimized version using numpy for faster computation.\n",
    "    \n",
    "    Args:\n",
    "        g_probs: List of probabilities g(y) for each fact y\n",
    "        b: Number of bins (natural number)\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples representing (bin_start, bin_end) in probability space\n",
    "    \"\"\"\n",
    "    if b < 1:\n",
    "        raise ValueError(\"Number of bins must be positive\")\n",
    "    \n",
    "    if not g_probs or all(p == 0 for p in g_probs):\n",
    "        return [(0, 1.0)]\n",
    "    \n",
    "    # Convert to numpy array for faster operations\n",
    "    g_probs_arr = np.array(g_probs)\n",
    "    total_mass = np.sum(g_probs_arr)\n",
    "    \n",
    "    if total_mass == 0:\n",
    "        return [(0, 1.0)]\n",
    "    \n",
    "    # Sort probabilities\n",
    "    sorted_probs = np.sort(g_probs_arr)\n",
    "    \n",
    "    # Calculate cumulative sum and normalize in one step\n",
    "    cumsum = np.cumsum(sorted_probs) / total_mass\n",
    "    \n",
    "    # Initialize bins\n",
    "    bins = []\n",
    "    bin_start_prob = 0\n",
    "    last_bin_end = None\n",
    "    \n",
    "    # Pre-calculate target cumulative sums\n",
    "    target_cumsums = np.linspace(1/b, 1, b)\n",
    "    \n",
    "    # Use binary search to find bin boundaries\n",
    "    for target in target_cumsums:\n",
    "        # Find index where cumsum exceeds target\n",
    "        j = bisect_right(cumsum, target)\n",
    "        \n",
    "        # Get bin end probability\n",
    "        if j >= len(sorted_probs):\n",
    "            bin_end_prob = 1.0\n",
    "        else:\n",
    "            bin_end_prob = sorted_probs[j]\n",
    "        \n",
    "        # Only add bin if it's different from the previous one\n",
    "        if bin_end_prob != last_bin_end:\n",
    "            bins.append((bin_start_prob, bin_end_prob))\n",
    "            bin_start_prob = bin_end_prob\n",
    "            last_bin_end = bin_end_prob\n",
    "    \n",
    "    # Ensure last bin goes up to 1.0 if not already included\n",
    "    if bins and bins[-1][1] != 1.0:\n",
    "        bins.append((bins[-1][1], 1.0))\n",
    "\n",
    "    #add last (1.0, 1.0) bin for edge case\n",
    "    bins.append((1.0, 1.0))\n",
    "        \n",
    "    return bins\n",
    "\n",
    "def miscalibration_calc(F_dataset, model, epsilon):\n",
    "    from collections import defaultdict\n",
    "    import math\n",
    "    all_facts = F_dataset\n",
    "    fact_counts = Counter(all_facts)\n",
    "    total_facts = len(all_facts)\n",
    "    # Calculate p(x) and g(x) for each fact\n",
    "    p_prob_list = []\n",
    "    g_prob_list = []\n",
    "    facts_list = []\n",
    "    \n",
    "    for fact in all_facts:\n",
    "       # p(x) probability\n",
    "       p_prob = fact_counts[fact] / total_facts\n",
    "       p_prob_list.append(p_prob)\n",
    "    \n",
    "       # Original g(x) probability\n",
    "       log_prob = model.calculate_log_probability(fact)\n",
    "       g_val = math.exp(log_prob)\n",
    "       g_prob_list.append(g_val)\n",
    "       \n",
    "       facts_list.append(fact)\n",
    "    \n",
    "    # Normalize g probabilities\n",
    "    g_sum = sum(g_prob_list)\n",
    "    # print(f\"Pre calibration, pre norm g prob list: {g_prob_list}\")\n",
    "    if g_sum > 0:\n",
    "       g_prob_list = [g / g_sum for g in g_prob_list]\n",
    "    \n",
    "    # Normalize p probabilities if needed\n",
    "    p_sum = sum(p_prob_list)\n",
    "    if p_sum > 0:\n",
    "       p_prob_list = [p / p_sum for p in p_prob_list]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Create bins and calculate pre-calibration metrics\n",
    "    # ---------------------------------------------------------\n",
    "    bins = create_epsilon_induced_bins(epsilon)\n",
    "    \n",
    "    if isinstance(bins, str) and bins == \"finest\":\n",
    "       unique_probs = sorted(set(g_prob_list), reverse=True)\n",
    "       bins = [(p, p) for p in unique_probs]\n",
    "    \n",
    "    # Assign facts to bins and calculate sums\n",
    "    binned_facts = [[] for _ in range(len(bins))]\n",
    "    binned_p_sums = [0.0] * len(bins)\n",
    "    binned_g_sums = [0.0] * len(bins)\n",
    "    \n",
    "    # Bin assignment\n",
    "    for i, (fact, g_val) in enumerate(zip(facts_list, g_prob_list)):\n",
    "       assigned = False\n",
    "       for bin_idx, (low, high) in enumerate(bins):\n",
    "          if epsilon != 0: \n",
    "               if low <= g_val < high:\n",
    "                   binned_facts[bin_idx].append((fact, g_val))\n",
    "                   binned_p_sums[bin_idx] += p_prob_list[i]\n",
    "                   binned_g_sums[bin_idx] += g_val\n",
    "                   assigned = True\n",
    "                   break\n",
    "          else:         \n",
    "               if low <= g_val <= high:\n",
    "                   binned_facts[bin_idx].append((fact, g_val))\n",
    "                   binned_p_sums[bin_idx] += p_prob_list[i]\n",
    "                   binned_g_sums[bin_idx] += g_val\n",
    "                   assigned = True\n",
    "                   break               \n",
    "       if not assigned:\n",
    "           last_idx = len(bins) - 1\n",
    "           binned_facts[last_idx].append((fact, g_val))\n",
    "           binned_p_sums[last_idx] += p_prob_list[i]\n",
    "           binned_g_sums[last_idx] += g_val\n",
    "    \n",
    "    miscalibration = 0.5 * sum(abs(binned_p_sums[i] - binned_g_sums[i]) \n",
    "                                 for i in range(len(bins)))\n",
    "    return miscalibration\n",
    "\n",
    "def calibrate_token_transitions(new_model, facts):\n",
    "    \"\"\"\n",
    "    Add transition counts from new facts to model without normalization.\n",
    "    \n",
    "    Args:\n",
    "        new_model: MovieFactMarkovChain instance\n",
    "        facts: List of new facts\n",
    "    \"\"\"\n",
    "    for fact in facts:\n",
    "        tokens = new_model.tokenize_fact(fact) + [new_model.EOS]\n",
    "        if len(tokens) < new_model.order:\n",
    "            continue\n",
    "            \n",
    "        # Add start sequence count\n",
    "        start_tuple = tuple(tokens[:new_model.order]) \n",
    "        new_model.initial_probs[start_tuple] = new_model.initial_probs.get(start_tuple, 0) + 1\n",
    "        \n",
    "        # Add transition counts\n",
    "        for i in range(len(tokens) - new_model.order):\n",
    "            current = tuple(tokens[i:i + new_model.order])\n",
    "            next_token = tokens[i + new_model.order]\n",
    "            if current not in new_model.transitions:\n",
    "                new_model.transitions[current] = defaultdict(int)\n",
    "            new_model.transitions[current][next_token] += 1\n",
    "            \n",
    "    # Update starts list\n",
    "    new_model.starts = list(set(new_model.starts + [tuple(f[:new_model.order]) for f in facts if len(f) >= new_model.order]))\n",
    "\n",
    "def normalize_transitions(transitions):\n",
    "  \"\"\"Helper function to normalize transition probabilities\"\"\"\n",
    "  total = sum(transitions.values())\n",
    "  if total > 0:\n",
    "     for token in transitions:\n",
    "        transitions[token] /= total\n",
    "  return transitions\n",
    "            \n",
    "def upweight_train(model, training_data, sample_to_change):\n",
    "   # -- 1) Initialize new model from input model --\n",
    "   new_model = MovieFactMarkovChain(order=model.order)\n",
    "   new_model.transitions = defaultdict(lambda: defaultdict(float))\n",
    "   new_model.initial_probs = defaultdict(float)\n",
    "\n",
    "   # Copy all transitions from input model\n",
    "   for state, next_dict in model.transitions.items():\n",
    "      new_model.transitions[state] = defaultdict(float, next_dict.copy())\n",
    "   new_model.initial_probs.update(model.initial_probs)\n",
    "\n",
    "   if sample_to_change == 0:\n",
    "       return model, sample_to_change\n",
    "\n",
    "   if sample_to_change <= len(training_data):\n",
    "       facts_in_bin = training_data[:sample_to_change+1]\n",
    "       calibrate_token_transitions(new_model, facts_in_bin)\n",
    "       \n",
    "   for state in new_model.transitions:\n",
    "      normalize_transitions(new_model.transitions[state])\n",
    "   init_total = sum(new_model.initial_probs.values())\n",
    "   if init_total > 0:\n",
    "      for k in new_model.initial_probs:\n",
    "          new_model.initial_probs[k] /= init_total\n",
    "   else: \n",
    "       return new_model, float('-inf')\n",
    "   return new_model, sample_to_change\n",
    "    \n",
    "def calculate_kalai_calibration_log_binning(F_dataset, training_data, model, epsilon, sample_to_change):\n",
    "   miscalibration_pre = miscalibration_calc(F_dataset, model, epsilon)\n",
    "\n",
    "   new_model, sample_to_change = upweight_train(model, training_data, sample_to_change)\n",
    "\n",
    "   miscalibration_post = miscalibration_calc(F_dataset, new_model, epsilon)\n",
    "    \n",
    "\n",
    "   return {\n",
    "       \"miscalibration_pre\": float(miscalibration_pre),\n",
    "       \"miscalibration_post\": float(miscalibration_post),\n",
    "       \"new_model\": new_model,\n",
    "       \"sample_changed\": sample_to_change}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a15844-14ad-43bc-a4e8-f9176adc7028",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0afe3-d942-4816-8810-b6888f8274b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execution\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(140)\n",
    "\n",
    "uniform_p = pd.read_json('path', orient='records')\n",
    "uniform_p = uniform_p[1:20000]\n",
    "# print(uniform_p)\n",
    "set_order = 2\n",
    "generation_size = 5000\n",
    "sample_size = 5000\n",
    "bin_num = 133\n",
    "epsilon = 0.1\n",
    "paretos_store = []\n",
    "num_bin_changes = []\n",
    "sample_changes = []\n",
    "\n",
    "hall_rates_pre = []\n",
    "hall_rates_post = []\n",
    "\n",
    "mono_pcts = []\n",
    "mono_pcts_p = []\n",
    "\n",
    "miscals_pre = []\n",
    "miscals_post = []\n",
    "\n",
    "alphas = [1.25, 1.5, 2.25, 2.5]\n",
    "\n",
    "total_iterations = 1\n",
    "progress_bar = tqdm(total=total_iterations * len(alphas), desc=\"Overall Progress\")\n",
    "\n",
    "for i in range(total_iterations):\n",
    "    for pareto_alpha in alphas:\n",
    "        # print(f\"\\nProcessing pareto_alpha: {pareto_alpha}\")\n",
    "        # p distribution\n",
    "        powerlaw_p = create_powerlaw_p(uniform_p, pareto_alpha)\n",
    "        # generation_size = int(len(powerlaw_p) * 0.25)\n",
    "        # sample_size = generation_size\n",
    "        # training data \n",
    "        training_data = sample(powerlaw_p, sample_size)\n",
    "        # mono rate calc\n",
    "        mono_rate = mono_calc(training_data)\n",
    "        mono_rate_p = mono_calc(powerlaw_p)\n",
    "        \n",
    "        # fit model\n",
    "        model = MovieFactMarkovChain(order=set_order)\n",
    "        model.train(training_data)\n",
    "    \n",
    "        for i in list(range(0, sample_size+1, 312)):\n",
    "            print(i)\n",
    "            mono_pcts.append(mono_rate)\n",
    "            mono_pcts_p.append(mono_rate_p)\n",
    "            paretos_store.append(pareto_alpha)\n",
    "            # print(f\"bin change: {i} bin(s)\")\n",
    "            generated_data = model.generate_facts(generation_size)\n",
    "            #hallucination results\n",
    "            hall_results_pre = check_hallucinations(generated_data, training_data, powerlaw_p)\n",
    "            hall_rates_pre.append(hall_results_pre['hallucination_rate'])\n",
    "            \n",
    "            #miscalibration results\n",
    "            miscal_results = calculate_kalai_calibration_log_binning(powerlaw_p, training_data, model, epsilon, i)\n",
    "            sample_changes.append(miscal_results[\"sample_changed\"])\n",
    "            miscals_pre.append(miscal_results[\"miscalibration_pre\"])\n",
    "            miscals_post.append(miscal_results[\"miscalibration_post\"])\n",
    "            new_model = miscal_results[\"new_model\"]\n",
    "    \n",
    "    \n",
    "            #early stopping\n",
    "            if miscal_results[\"sample_changed\"] >= sample_size:\n",
    "                generated_data_new = new_model.generate_facts(generation_size)\n",
    "                hall_results_post = check_hallucinations(generated_data_new, training_data, powerlaw_p)\n",
    "                hall_rates_post.append(hall_results_post['hallucination_rate']) \n",
    "                break\n",
    "            #new hallucination\n",
    "            generated_data_new = new_model.generate_facts(generation_size)\n",
    "            hall_results_post = check_hallucinations(generated_data_new, training_data, powerlaw_p)\n",
    "            hall_rates_post.append(hall_results_post['hallucination_rate'])\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    results_df= pd.DataFrame({\n",
    "        'Mono_Pct': mono_pcts,\n",
    "        \"Miscalibration_Pre\": miscals_pre,\n",
    "        \"Miscalibration_Post\": miscals_post,\n",
    "        \"Hallucination_Pre\": hall_rates_pre,\n",
    "        \"Hallucination_Post\": hall_rates_post,\n",
    "        \"Pareto\": paretos_store,\n",
    "        \"Sample_Changed\": sample_changes\n",
    "        })\n",
    "\n",
    "results_df.to_csv(\"path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3ec9a-6a47-4e6d-b920-6b5c5ab650f8",
   "metadata": {},
   "source": [
    "## Overall Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09fff7-ee80-4055-9285-8f0187243f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Set font to Times New Roman and increase font size to 11pt for legibility\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11,\n",
    "    'font.family': 'Times New Roman',\n",
    "    'figure.dpi': 300,  # Higher DPI for better quality\n",
    "    'lines.linewidth': 1.0,  # Ensure lines are at least 0.5 points thick\n",
    "    'axes.linewidth': 1.0,  # Make axis lines visible\n",
    "    'figure.facecolor': 'white',  # Ensure white background\n",
    "    'axes.facecolor': 'white'\n",
    "})\n",
    "\n",
    "def create_icml_figure(results_df, output_path):\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 6))\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    # Define colors and styles\n",
    "    primary_color = 'blue'\n",
    "    secondary_color = 'red'\n",
    "    pre_linestyle = (0, (5, 5))  # Dashed\n",
    "    post_linestyle = 'solid'\n",
    "    \n",
    "    # Pareto values\n",
    "    pareto_values = [1, 1.25, 1.5, 2, 2.25, 4]\n",
    "    \n",
    "    for idx, pareto_val in enumerate(pareto_values):\n",
    "        mask = (results_df['Pareto'] == pareto_val)\n",
    "        pareto_data = results_df[mask].copy()\n",
    "        \n",
    "        ax2 = axs[idx].twinx()\n",
    "        \n",
    "        # Initialize median_mono as NaN; will update if we have valid data\n",
    "        median_mono = np.nan\n",
    "        \n",
    "        if len(pareto_data) > 0:\n",
    "            # Sort data by Sample_Changed to ensure proper plotting\n",
    "            pareto_data = pareto_data.sort_values('Sample_Changed')\n",
    "            \n",
    "            # If we have a Mono_Pct column, extract the (unique) monofact rate\n",
    "            if 'Mono_Pct' in pareto_data.columns:\n",
    "                # Assuming all rows for this Pareto share the same Mono_Pct\n",
    "                median_mono = pareto_data['Mono_Pct'].iloc[0]\n",
    "            \n",
    "            # Plot Miscalibration\n",
    "            axs[idx].plot(pareto_data['Sample_Changed'], \n",
    "                          pareto_data['Miscalibration_Post'],\n",
    "                          color=primary_color, linestyle=post_linestyle,\n",
    "                          label='Post-upweighting', linewidth=1.5)\n",
    "            \n",
    "            axs[idx].plot(pareto_data['Sample_Changed'], \n",
    "                          pareto_data['Miscalibration_Pre'],\n",
    "                          color=primary_color, linestyle=pre_linestyle,\n",
    "                          label='Original', linewidth=1.5)\n",
    "            \n",
    "            # Plot Hallucination\n",
    "            ax2.plot(pareto_data['Sample_Changed'], \n",
    "                     pareto_data['Hallucination_Post'],\n",
    "                     color=secondary_color, linestyle=post_linestyle,\n",
    "                     label='Post-upweighting', linewidth=1.5)\n",
    "            \n",
    "            ax2.plot(pareto_data['Sample_Changed'], \n",
    "                     pareto_data['Hallucination_Pre'],\n",
    "                     color=secondary_color, linestyle=pre_linestyle,\n",
    "                     label='Original', linewidth=1.5)\n",
    "            \n",
    "            # Axes limits and labels\n",
    "            axs[idx].set_xlim(0, 5000)\n",
    "            axs[idx].set_xlabel('Number of Upweighted Training Examples')\n",
    "            axs[idx].set_ylabel('Miscalibration Rate', color=primary_color)\n",
    "            ax2.set_ylabel('Hallucination Rate', color=secondary_color)\n",
    "            \n",
    "            # Ticks\n",
    "            x_ticks = np.arange(0, 5001, 1000)\n",
    "            axs[idx].set_xticks(x_ticks)\n",
    "            axs[idx].tick_params(axis='y', labelcolor=primary_color)\n",
    "            ax2.tick_params(axis='y', labelcolor=secondary_color)\n",
    "        \n",
    "        # Subplot title\n",
    "        title = f'Pareto Shape Parameter: {pareto_val}'\n",
    "        if not np.isnan(median_mono):\n",
    "            title += f' (Monofact Rate: {median_mono:.3f})'\n",
    "        axs[idx].set_title(title, fontsize=11)\n",
    "    \n",
    "    # Create a unified legend for all subplots\n",
    "    lines = [\n",
    "        Line2D([0], [0], color=primary_color, linestyle=post_linestyle, \n",
    "               label='Miscalibration Post Upweighting'),\n",
    "        Line2D([0], [0], color=primary_color, linestyle=pre_linestyle, \n",
    "               label='Original Miscalibration'),\n",
    "        Line2D([0], [0], color=secondary_color, linestyle=post_linestyle, \n",
    "               label='Hallucination Post Upweighting'),\n",
    "        Line2D([0], [0], color=secondary_color, linestyle=pre_linestyle, \n",
    "               label='Original Hallucination')\n",
    "    ]\n",
    "    \n",
    "    fig.legend(handles=lines, loc='center', bbox_to_anchor=(0.5, 0.98),\n",
    "               ncol=4, fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for the legend\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "results_df = pd.read_csv(\"path\")\n",
    "results_df = results_df.copy()\n",
    "results_df['Hallucination_Pre'] = results_df.groupby('Pareto')['Hallucination_Pre'].transform('first')\n",
    "create_icml_figure(results_df, \"path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271133a-db11-489b-8bd1-042613d11b05",
   "metadata": {},
   "source": [
    "## Main Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4939904-9fe7-43aa-9dc4-bc123411635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Set font to Times New Roman and increase font size to 11pt for legibility\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11,\n",
    "    'font.family': 'Times New Roman',\n",
    "    'figure.dpi': 300,  # Higher DPI for better quality\n",
    "    'lines.linewidth': 1.0,  # Ensure lines are at least 0.5 points thick\n",
    "    'axes.linewidth': 1.0,   # Make axis lines visible\n",
    "    'figure.facecolor': 'white',  # Ensure white background\n",
    "    'axes.facecolor': 'white'\n",
    "})\n",
    "\n",
    "def create_icml_figure(results_df, output_path):\n",
    "    # Create figure and axis objects\n",
    "    # Here, figsize=(12, 2) is quite \"short,\" so we may need\n",
    "    # to give extra top margin for the legend\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    # Define colors and styles\n",
    "    primary_color = 'blue'\n",
    "    secondary_color = 'red'\n",
    "    pre_linestyle = (0, (5, 5))  # Dashed\n",
    "    post_linestyle = 'solid'\n",
    "    \n",
    "    # Pareto values\n",
    "    pareto_values = [1, 1.5]\n",
    "    \n",
    "    for idx, pareto_val in enumerate(pareto_values):\n",
    "        mask = (results_df['Pareto'] == pareto_val)\n",
    "        pareto_data = results_df[mask].copy()\n",
    "        \n",
    "        ax2 = axs[idx].twinx()\n",
    "        \n",
    "        # Initialize median_mono as NaN; will update if we have valid data\n",
    "        median_mono = np.nan\n",
    "        \n",
    "        if len(pareto_data) > 0:\n",
    "            # Sort data by Sample_Changed to ensure proper plotting\n",
    "            pareto_data = pareto_data.sort_values('Sample_Changed')\n",
    "            \n",
    "            # If we have a Mono_Pct column, extract the (unique) monofact rate\n",
    "            if 'Mono_Pct' in pareto_data.columns:\n",
    "                # Assuming all rows for this Pareto share the same Mono_Pct\n",
    "                median_mono = pareto_data['Mono_Pct'].iloc[0]\n",
    "            \n",
    "            # Plot Miscalibration\n",
    "            axs[idx].plot(pareto_data['Sample_Changed'], \n",
    "                          pareto_data['Miscalibration_Post'],\n",
    "                          color=primary_color, linestyle=post_linestyle,\n",
    "                          label='Post-upweighting', linewidth=1.5)\n",
    "            \n",
    "            axs[idx].plot(pareto_data['Sample_Changed'], \n",
    "                          pareto_data['Miscalibration_Pre'],\n",
    "                          color=primary_color, linestyle=pre_linestyle,\n",
    "                          label='Original', linewidth=1.5)\n",
    "            \n",
    "            # Plot Hallucination\n",
    "            ax2.plot(pareto_data['Sample_Changed'], \n",
    "                     pareto_data['Hallucination_Post'],\n",
    "                     color=secondary_color, linestyle=post_linestyle,\n",
    "                     label='Post-upweighting', linewidth=1.5)\n",
    "            \n",
    "            ax2.plot(pareto_data['Sample_Changed'], \n",
    "                     pareto_data['Hallucination_Pre'],\n",
    "                     color=secondary_color, linestyle=pre_linestyle,\n",
    "                     label='Original', linewidth=1.5)\n",
    "            \n",
    "            # Axes limits and labels\n",
    "            axs[idx].set_xlim(0, 5000)\n",
    "            axs[idx].set_xlabel('Number of Upweighted Training Examples')\n",
    "            axs[idx].set_ylabel('Miscalibration Rate', color=primary_color)\n",
    "            ax2.set_ylabel('Hallucination Rate', color=secondary_color)\n",
    "            \n",
    "            # Ticks\n",
    "            x_ticks = np.arange(0, 5001, 1000)\n",
    "            axs[idx].set_xticks(x_ticks)\n",
    "            axs[idx].tick_params(axis='y', labelcolor=primary_color)\n",
    "            ax2.tick_params(axis='y', labelcolor=secondary_color)\n",
    "        \n",
    "        # Subplot title\n",
    "        title = f'Pareto Shape Parameter: {pareto_val}'\n",
    "        if not np.isnan(median_mono):\n",
    "            title += f' (Monofact Rate: {median_mono:.3f})'\n",
    "        axs[idx].set_title(title, fontsize=11)\n",
    "    \n",
    "    # Create a unified legend for all subplots\n",
    "    lines = [\n",
    "        Line2D([0], [0], color=primary_color, linestyle=post_linestyle, \n",
    "               label='Miscalibration Post Upweighting'),\n",
    "        Line2D([0], [0], color=primary_color, linestyle=pre_linestyle, \n",
    "               label='Original Miscalibration'),\n",
    "        Line2D([0], [0], color=secondary_color, linestyle=post_linestyle, \n",
    "               label='Hallucination Post Upweighting'),\n",
    "        Line2D([0], [0], color=secondary_color, linestyle=pre_linestyle, \n",
    "               label='Original Hallucination')\n",
    "    ]\n",
    "    \n",
    "    # Place the legend above the subplots, giving extra vertical space\n",
    "    fig.legend(\n",
    "        handles=lines,\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 0.85),  # Higher Y-value => more space\n",
    "        ncol=4,\n",
    "        fontsize=11,\n",
    "        borderaxespad=0.2\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.65)\n",
    "    \n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# execution\n",
    "results_df = pd.read_csv(\"path\")\n",
    "results_df = results_df.copy()\n",
    "results_df['Hallucination_Pre'] = results_df.groupby('Pareto')['Hallucination_Pre'].transform('first')\n",
    "create_icml_figure(results_df, \"path\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
