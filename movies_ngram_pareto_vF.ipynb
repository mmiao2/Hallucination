{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e36c766-d408-4520-a979-af51a2eaecba",
   "metadata": {},
   "source": [
    "## Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7579b-fc13-4030-b08c-11af1c10b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "from collections import Counter\n",
    "from scipy.stats import pareto\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, kurtosis\n",
    "random.seed(1217)\n",
    "F_dataset = pd.read_json('path', orient='records')\n",
    "def create_powerlaw_p(F_dataset, pareto_alpha):\n",
    "    #create p\n",
    "    all_facts = F_dataset['fact'].tolist()\n",
    "    new_facts = []\n",
    "    for fact in all_facts:\n",
    "        reps = pareto.rvs(b=pareto_alpha, scale=1)\n",
    "        reps = int(np.floor(reps))\n",
    "        new_facts.extend([fact] * reps)\n",
    "    return new_facts\n",
    "\n",
    "def create_uniform_p(F_dataset):\n",
    "    #create p, which is already uniform\n",
    "    new_facts = F_dataset['fact'].tolist()\n",
    "    return new_facts\n",
    "\n",
    "def sample(new_facts, size):\n",
    "    #sample with replacement\n",
    "    training_data = random.sample(new_facts, k=size)\n",
    "    return training_data\n",
    "\n",
    "def mono_calc(new_facts):\n",
    "    fact_counts = Counter(new_facts)\n",
    "    num_mono = sum(1 for count in fact_counts.values() if count == 1)\n",
    "    mono_pct = num_mono / len(new_facts)\n",
    "    return mono_pct\n",
    "    \n",
    "class MovieFactMarkovChain:\n",
    "    def __init__(self, order):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(lambda: defaultdict(float))\n",
    "        self.initial_counts = defaultdict(float)\n",
    "        self.starts = []\n",
    "        self.EOS = '<EOS>'\n",
    "        \n",
    "    def tokenize_fact(self, fact):\n",
    "        # Modify this method if city facts have a specific format\n",
    "        return fact.strip().split()\n",
    "    \n",
    "    def calculate_log_probability(self, fact):\n",
    "        \"\"\"Calculate log probability for a fact using order-n transitions.\"\"\"\n",
    "        tokens = self.tokenize_fact(fact) + [self.EOS]\n",
    "        if len(tokens) < self.order:\n",
    "            return np.log(1e-10)\n",
    "        \n",
    "        # Start with initial probability in log-space\n",
    "        start_tokens = tuple(tokens[:self.order])\n",
    "        log_probability = np.log(self.initial_probs.get(start_tokens, 1e-10))\n",
    "        \n",
    "        # Use order-n transitions\n",
    "        for i in range(len(tokens) - self.order):\n",
    "            current = tuple(tokens[i:i + self.order])\n",
    "            next_token = tokens[i + self.order]\n",
    "            \n",
    "            # Get transition probability\n",
    "            trans_prob = self.transitions.get(current, {}).get(next_token, 1e-10)\n",
    "            log_probability += np.log(trans_prob)\n",
    "        \n",
    "        return log_probability\n",
    "    \n",
    "    def train(self, facts):\n",
    "        # print(f\"\\nTraining {self.order}-order Markov Chain...\")\n",
    "        num_facts = len(facts)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # First pass: collect transitions\n",
    "        # print(\"Collecting transitions...\")\n",
    "        for fact in tqdm(facts, total=num_facts):\n",
    "            tokens = self.tokenize_fact(fact) + [self.EOS]\n",
    "            if len(tokens) < self.order + 1:\n",
    "                continue  # Skip if not enough tokens\n",
    "            \n",
    "            start_tokens = tuple(tokens[:self.order])\n",
    "            self.starts.append(start_tokens)\n",
    "            self.initial_counts[start_tokens] += 1\n",
    "            \n",
    "            for i in range(len(tokens) - self.order):\n",
    "                current = tuple(tokens[i:i + self.order])\n",
    "                next_token = tokens[i + self.order]\n",
    "                self.transitions[current][next_token] += 1\n",
    "        \n",
    "        total_starts = sum(self.initial_counts.values())\n",
    "        self.initial_probs = {start: count / total_starts for start, count in self.initial_counts.items()}\n",
    "        \n",
    "        for current, next_tokens in tqdm(self.transitions.items()):\n",
    "            total = sum(next_tokens.values())\n",
    "            for next_token in next_tokens:\n",
    "                next_tokens[next_token] /= total\n",
    "                    \n",
    "        training_time = time.time() - start_time\n",
    "    \n",
    "    def generate_facts(self, num_generations, output_path):\n",
    "        \"\"\"Generate new facts\"\"\"\n",
    "        generated_facts = []\n",
    "        # print(f\"\\nGenerating {num_generations:,} facts...\")\n",
    "        \n",
    "        with tqdm(total=num_generations) as pbar:\n",
    "            while len(generated_facts) < num_generations:\n",
    "                # Pick a random start sequence\n",
    "                current = random.choices(\n",
    "                    population=list(self.initial_probs.keys()),\n",
    "                    weights=list(self.initial_probs.values()),\n",
    "                    k=1\n",
    "                )[0]\n",
    "                result = list(current)\n",
    "                \n",
    "                # Generate until EOS token is produced\n",
    "                while True:\n",
    "                    if current not in self.transitions:\n",
    "                        break  # Can't continue from here\n",
    "                    \n",
    "                    # Get possible next tokens and their probabilities\n",
    "                    possible_tokens = list(self.transitions[current].keys())\n",
    "                    probabilities = list(self.transitions[current].values())\n",
    "                    \n",
    "                    # Generate next token\n",
    "                    next_token = np.random.choice(possible_tokens, p=probabilities)\n",
    "                    if next_token == self.EOS:\n",
    "                        break  # End of sequence\n",
    "                    result.append(next_token)\n",
    "                    \n",
    "                    # Update current context\n",
    "                    current = tuple(result[-self.order:])\n",
    "                \n",
    "                # Add generated fact\n",
    "                generated_fact = ' '.join(result)\n",
    "                generated_facts.append({\n",
    "                    'fact': generated_fact,\n",
    "                    'generated': 1\n",
    "                })\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Create DataFrame and save\n",
    "            generated_df = pd.DataFrame(generated_facts)\n",
    "            return generated_df\n",
    "\n",
    "#-----------------------------------------check hallucinations and calibration-----------------------------------------------\n",
    "def check_hallucinations(generated_data, training_data, F_dataset):\n",
    "    \n",
    "    generated_facts = set(generated_data['fact'])\n",
    "    F_facts = set(F_dataset)  # All true facts\n",
    "    \n",
    "    # Hallucinations are generated facts not in F\n",
    "    hallucinations = list(generated_facts - F_facts)\n",
    "    hallucination_rate = len(hallucinations) / len(generated_data)\n",
    "    \n",
    "    # Also calculate \"unseen but true\" rate\n",
    "    training_facts = set(training_data)\n",
    "    unseen_true = list(generated_facts - training_facts - set(hallucinations))\n",
    "    unseen_true_rate = len(unseen_true) / len(generated_data)\n",
    "    \n",
    "    return {\n",
    "        'hallucination_rate': hallucination_rate,\n",
    "        'hallucinated_facts': hallucinations,\n",
    "        'unseen_true_rate': unseen_true_rate,\n",
    "        'unseen_true_facts': unseen_true\n",
    "    }\n",
    "\n",
    "def create_epsilon_induced_bins(epsilon: float):\n",
    "   if epsilon < 0:\n",
    "       raise ValueError(\"Epsilon must be non-negative\")\n",
    "   \n",
    "   if epsilon == 0:\n",
    "       return \"finest\"\n",
    "   \n",
    "   if epsilon >= 1:\n",
    "       # When epsilon = 1, return single bin for all probabilities\n",
    "       return [(0, 1)]\n",
    "   \n",
    "   bins = []\n",
    "   i = 0\n",
    "   \n",
    "   while True:\n",
    "       upper = (1-epsilon)**i\n",
    "       lower = (1-epsilon)**(i+1)\n",
    "       \n",
    "       # If lower bound gets very small, make it 0 and make this our last bin\n",
    "       if lower < 1e-10:\n",
    "           bins.append((0, upper))\n",
    "           break\n",
    "           \n",
    "       if upper - lower > 1e-10:\n",
    "           bins.append((lower, upper))\n",
    "       \n",
    "       i += 1\n",
    "   ##append final bin for edge case where everything is 1\n",
    "   bins.append((1.0, 1.0))\n",
    "   return bins\n",
    "    \n",
    "def calculate_kalai_calibration_log_binning(F_dataset, training, model, epsilon, output_path):  \n",
    "    # Define training facts from O_dataset and get frequencies\n",
    "    all_facts = F_dataset\n",
    "    fact_counts = Counter(all_facts)\n",
    "    total_facts = len(all_facts)\n",
    "    \n",
    "    # print(f\"\\nCalculating g(y) probabilities for {total_facts} training facts...\")\n",
    "    \n",
    "    facts_list = []\n",
    "    g_prob_list = []\n",
    "    p_prob_list = []\n",
    "    \n",
    "    raw_g_prob_list = []\n",
    "    raw_p_prob_list = []\n",
    "    \n",
    "    for fact in tqdm(all_facts):\n",
    "        log_prob = model.calculate_log_probability(fact)\n",
    "        g_prob = np.exp(log_prob)\n",
    "\n",
    "        facts_list.append(fact)\n",
    "        g_prob_list.append(g_prob)\n",
    "        raw_g_prob_list.append(g_prob)\n",
    "        \n",
    "        p_prob = fact_counts[fact] / total_facts\n",
    "        p_prob_list.append(p_prob)\n",
    "        raw_p_prob_list.append(p_prob)\n",
    "\n",
    "    # Normalize g(y) and p(y)\n",
    "    g_sum = sum(g_prob_list)\n",
    "    if g_sum > 0:\n",
    "        g_prob_list = [p / g_sum for p in g_prob_list]\n",
    "    p_sum = sum(p_prob_list)\n",
    "    if p_sum > 0:\n",
    "        p_prob_list = [p / p_sum for p in p_prob_list]\n",
    "\n",
    "    # Create bins\n",
    "    bins = create_epsilon_induced_bins(epsilon)\n",
    "    if isinstance(bins, str) and bins == \"finest\":\n",
    "        unique_probs = sorted(set(g_prob_list), reverse=True)\n",
    "        bins = [(p, p) for p in unique_probs]\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # 1) Create data structures to accumulate bin memberships + sums.\n",
    "    # ----------------------------------------------------------------\n",
    "    binned_facts = [[] for _ in range(len(bins))]\n",
    "\n",
    "    # Keep track of the sum of p(y) and g(y) in each bin\n",
    "    binned_p_sums = [0.0] * len(bins)\n",
    "    binned_g_sums = [0.0] * len(bins)\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # 2) Assign each fact to exactly one bin and update running sums.\n",
    "    # -------------------------------------------------------------\n",
    "    for i, (fact, g_y) in enumerate(zip(facts_list, g_prob_list)):\n",
    "        assigned = False\n",
    "        for bin_idx, (lower, upper) in enumerate(bins):\n",
    "            # If it falls into [lower, upper)\n",
    "            if lower <= g_y < upper:\n",
    "                binned_facts[bin_idx].append((fact, g_y))\n",
    "                binned_p_sums[bin_idx] += p_prob_list[i]\n",
    "                binned_g_sums[bin_idx] += g_prob_list[i]\n",
    "                assigned = True\n",
    "                break\n",
    "        # If we never broke out, it goes to the last bin\n",
    "        # (Handles edge-case if upper=1 for last bin.)\n",
    "        if not assigned:\n",
    "            last_idx = len(bins) - 1\n",
    "            binned_facts[last_idx].append((fact, g_y))\n",
    "            binned_p_sums[last_idx] += p_prob_list[i]\n",
    "            binned_g_sums[last_idx] += g_prob_list[i]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # 3) Calculate miscalibration efficiently\n",
    "    # ---------------------------------------\n",
    "    miscalibration = 0.0\n",
    "    for bin_idx in range(len(bins)):\n",
    "        p_B = binned_p_sums[bin_idx]\n",
    "        g_B = binned_g_sums[bin_idx]\n",
    "        miscalibration += abs(p_B - g_B)\n",
    "    \n",
    "    miscalibration *= 0.5\n",
    "    \n",
    "    # Calculate log loss\n",
    "    log_loss = 0\n",
    "    for fact in training:\n",
    "        g_prob_train = np.exp(model.calculate_log_probability(fact))\n",
    "        if g_prob_train > 0:\n",
    "            log_loss += np.log(1/g_prob_train)\n",
    "    \n",
    "    return {\n",
    "        'miscalibration': float(miscalibration),\n",
    "        'num_bins': len(binned_facts),\n",
    "        'log_loss': float(log_loss),\n",
    "    }\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875c4e5-b393-42a4-94b6-3b5cab958c4e",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36ddd4-4858-46ee-89e9-0a8dc853772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(140)\n",
    "\n",
    "uniform_p = pd.read_json('path', orient='records')\n",
    "uniform_p = uniform_p[1:20000]\n",
    "print(uniform_p)\n",
    "set_order = 2\n",
    "sample_size = 5000\n",
    "generation_size = 5000\n",
    "bin_num = 25\n",
    "epsilon = 0.1\n",
    "\n",
    "hall_rates = []\n",
    "unseen_true_rates = []\n",
    "pareto_alphas = []\n",
    "mono_pcts = []\n",
    "miscals = []\n",
    "lengths_train = []\n",
    "entropies = []\n",
    "\n",
    "max_train_repeats  = []\n",
    "average_train_repeats = []\n",
    "twenty_fifths = []\n",
    "medians = []\n",
    "seventy_fifths = []\n",
    "num_uniques = []\n",
    "\n",
    "max_repeats_p  = []\n",
    "average_repeats_p = []\n",
    "twenty_fifths_p = []\n",
    "medians_p = []\n",
    "seventy_fifths_p = []\n",
    "num_uniques_p = []\n",
    "\n",
    "\n",
    "mono_pcts_p = []\n",
    "mono_miscal_ds = []\n",
    "\n",
    "alphas = np.arange(1, 4, 0.5).tolist()\n",
    "for pareto_alpha in alphas:\n",
    "    print(f\"\\nProcessing pareto_alpha: {pareto_alpha}\")\n",
    "    # p distribution\n",
    "    \n",
    "    powerlaw_p = create_powerlaw_p(uniform_p, pareto_alpha)\n",
    "    training_data = sample(powerlaw_p, sample_size)\n",
    "\n",
    "    # check average and max repeats in training data\n",
    "    facts_counter = Counter(training_data)\n",
    "    repeats = list(facts_counter.values())\n",
    "    max_repeat = max(repeats)\n",
    "    avg_repeat = sum(repeats) / len(facts_counter)\n",
    "    twenty_fifth = np.percentile(repeats, 25)\n",
    "    median = np.percentile(repeats, 50)\n",
    "    seventy_fifth = np.percentile(repeats, 75)\n",
    "    num_unique = len(facts_counter.keys())\n",
    "    # mono rate calc\n",
    "    mono_rate = mono_calc(training_data)\n",
    "\n",
    "\n",
    "    #check average and max repeats in p \n",
    "    facts_counter_p = Counter(powerlaw_p)\n",
    "    repeats_p = list(facts_counter_p.values())\n",
    "    max_repeat_p = max(repeats_p)\n",
    "    avg_repeat_p = sum(repeats_p) / len(facts_counter_p)\n",
    "    twenty_fifth_p = np.percentile(repeats_p, 25)\n",
    "    median_p = np.percentile(repeats_p, 50)\n",
    "    seventy_fifth_p = np.percentile(repeats_p, 75)\n",
    "    num_unique_p = len(facts_counter_p.keys())\n",
    "    mono_rate_p = mono_calc(powerlaw_p)\n",
    "    \n",
    "    # fit model\n",
    "    model = MovieFactMarkovChain(order=set_order)\n",
    "    model.train(training_data)\n",
    "    \n",
    "    #generation path for fact checking\n",
    "    gen_path = f\"path\"\n",
    "    os.makedirs(os.path.dirname(gen_path), exist_ok=True)\n",
    "    generated_data = model.generate_facts(generation_size, gen_path)\n",
    "    #hallucination results\n",
    "    hall_results = check_hallucinations(generated_data, training_data, powerlaw_p)\n",
    "\n",
    "    #miscalibration results\n",
    "    miscal_path = f\"path\"\n",
    "    miscal_results = calculate_kalai_calibration_log_binning(powerlaw_p, model, epsilon, miscal_path)\n",
    "\n",
    "    mono_miscal_d = (abs(mono_rate - miscal_results['miscalibration']) * 100) / (mono_rate * 100)\n",
    "    mono_miscal_ds.append(mono_miscal_d)\n",
    "    #save results\n",
    "    mono_pcts.append(mono_rate)\n",
    "    mono_pcts_p.append(mono_rate_p)\n",
    "    \n",
    "    hall_rates.append(hall_results['hallucination_rate'])\n",
    "    unseen_true_rates.append(hall_results['unseen_true_rate'])\n",
    "    miscals.append(miscal_results['miscalibration'])\n",
    "    lengths_train.append(len(training_data))\n",
    "    pareto_alphas.append(pareto_alpha)\n",
    "    ## training repeats distribution\n",
    "    max_train_repeats.append(max_repeat)\n",
    "    average_train_repeats.append(avg_repeat)\n",
    "    twenty_fifths.append(twenty_fifth)\n",
    "    medians.append(median)\n",
    "    seventy_fifths.append(seventy_fifth)\n",
    "    num_uniques.append(num_unique)\n",
    "    ## p repeats distribution\n",
    "    max_repeats_p.append(max_repeat_p)\n",
    "    average_repeats_p.append(avg_repeat_p)\n",
    "    twenty_fifths_p.append(twenty_fifth_p)\n",
    "    medians_p.append(median_p)\n",
    "    seventy_fifths_p.append(seventy_fifth_p)\n",
    "    num_uniques_p.append(num_unique_p)  \n",
    "    \n",
    "results_df= pd.DataFrame({\n",
    "    \"Pareto\": pareto_alphas,\n",
    "    'Mono_Pct': mono_pcts,\n",
    "    'Mono_Pct_P': mono_pcts_p,\n",
    "    'Mono_Miscal_D': mono_miscal_ds, \n",
    "    \"Miscalibration\": miscals,\n",
    "    \"Hallucination\": hall_rates,\n",
    "    \"Unseen_True_Rates\": unseen_true_rates,\n",
    "    \"Num_Training_Facts\": lengths_train,\n",
    "    \"Max_Train_Repeats\": max_train_repeats,\n",
    "    \"Avg_Train_Repeats\": average_train_repeats,\n",
    "    \"25_Train_Repeats\": twenty_fifths,\n",
    "    \"50_Train_Repeats\": medians,\n",
    "    \"70_Train_Repeats\": seventy_fifths,\n",
    "    \"Max_P_Repeats\": max_repeats_p,\n",
    "    \"Avg_P_Repeats\": average_repeats_p,\n",
    "    \"25_P_Repeats\": twenty_fifths_p,\n",
    "    \"50_P_Repeats\": medians_p,\n",
    "    \"70_P_Repeats\": seventy_fifths_p,\n",
    "    \"Num_Unique_Train\": num_uniques, \n",
    "    \"Num_Unique_P\": num_uniques_p\n",
    "    })\n",
    "\n",
    "# Print summary statistics\n",
    "results_df.to_csv(\"path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903f72c-dda7-4bce-beea-dd84bb6b4dec",
   "metadata": {},
   "source": [
    "## Model Capacity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5091cc5-282a-4ede-b71d-fa2c21f8810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from scipy.stats import kurtosis\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(140)\n",
    "\n",
    "uniform_p = pd.read_json('path', orient='records')\n",
    "uniform_p = uniform_p[1:20000]\n",
    "print(uniform_p)\n",
    "set_orders = [1, 2, 3, 4, 5, 6]\n",
    "sample_size = 5000\n",
    "generation_size = 5000\n",
    "bin_num = 25\n",
    "epsilon = 0.1\n",
    "\n",
    "hall_rates = []\n",
    "unseen_true_rates = []\n",
    "pareto_alphas = []\n",
    "mono_pcts = []\n",
    "miscals = []\n",
    "lengths_train = []\n",
    "entropies = []\n",
    "\n",
    "max_train_repeats  = []\n",
    "average_train_repeats = []\n",
    "twenty_fifths = []\n",
    "medians = []\n",
    "seventy_fifths = []\n",
    "num_uniques = []\n",
    "\n",
    "max_repeats_p  = []\n",
    "average_repeats_p = []\n",
    "twenty_fifths_p = []\n",
    "medians_p = []\n",
    "seventy_fifths_p = []\n",
    "num_uniques_p = []\n",
    "mono_pcts_p = []\n",
    "\n",
    "capacities = []\n",
    "\n",
    "alphas = np.arange(1, 4, 0.025).tolist()\n",
    "for pareto_alpha in alphas:\n",
    "    print(f\"\\nProcessing pareto_alpha: {pareto_alpha}\")\n",
    "    # p distribution\n",
    "    for set_order in set_orders:\n",
    "        powerlaw_p = create_powerlaw_p(uniform_p, pareto_alpha)\n",
    "        training_data = sample(powerlaw_p, sample_size)\n",
    "\n",
    "        capacities.append(set_order)\n",
    "        # check average and max repeats in training data\n",
    "        facts_counter = Counter(training_data)\n",
    "        repeats = list(facts_counter.values())\n",
    "        max_repeat = max(repeats)\n",
    "        avg_repeat = sum(repeats) / len(facts_counter)\n",
    "        twenty_fifth = np.percentile(repeats, 25)\n",
    "        median = np.percentile(repeats, 50)\n",
    "        seventy_fifth = np.percentile(repeats, 75)\n",
    "        num_unique = len(facts_counter.keys())\n",
    "        # mono rate calc\n",
    "        mono_rate = mono_calc(training_data)\n",
    "\n",
    "    \n",
    "        #check average and max repeats in p \n",
    "        facts_counter_p = Counter(powerlaw_p)\n",
    "        repeats_p = list(facts_counter_p.values())\n",
    "        max_repeat_p = max(repeats_p)\n",
    "        avg_repeat_p = sum(repeats_p) / len(facts_counter_p)\n",
    "        twenty_fifth_p = np.percentile(repeats_p, 25)\n",
    "        median_p = np.percentile(repeats_p, 50)\n",
    "        seventy_fifth_p = np.percentile(repeats_p, 75)\n",
    "        num_unique_p = len(facts_counter_p.keys())\n",
    "        mono_rate_p = mono_calc(powerlaw_p)\n",
    "        \n",
    "        # fit model\n",
    "        model = MovieFactMarkovChain(order=set_order)\n",
    "        model.train(training_data)\n",
    "        \n",
    "        #generation path for fact checking\n",
    "        gen_path = f\"path\"\n",
    "        os.makedirs(os.path.dirname(gen_path), exist_ok=True)\n",
    "\n",
    "        generated_data = model.generate_facts(generation_size, gen_path)\n",
    "        #hallucination results\n",
    "        hall_results = check_hallucinations(generated_data, training_data, powerlaw_p)\n",
    "    \n",
    "        #miscalibration results\n",
    "        miscal_path = f\"path\"\n",
    "        miscal_results = calculate_kalai_calibration_log_binning(powerlaw_p, training_data, model, epsilon, miscal_path)\n",
    "    \n",
    "        #save results\n",
    "        mono_pcts.append(mono_rate)\n",
    "        mono_pcts_p.append(mono_rate_p)\n",
    "        \n",
    "        hall_rates.append(hall_results['hallucination_rate'])\n",
    "        unseen_true_rates.append(hall_results['unseen_true_rate'])\n",
    "        miscals.append(miscal_results['miscalibration'])\n",
    "        lengths_train.append(len(training_data))\n",
    "        pareto_alphas.append(pareto_alpha)\n",
    "        ## training repeats distribution\n",
    "        max_train_repeats.append(max_repeat)\n",
    "        average_train_repeats.append(avg_repeat)\n",
    "        twenty_fifths.append(twenty_fifth)\n",
    "        medians.append(median)\n",
    "        seventy_fifths.append(seventy_fifth)\n",
    "        num_uniques.append(num_unique)\n",
    "        ## p repeats distribution\n",
    "        max_repeats_p.append(max_repeat_p)\n",
    "        average_repeats_p.append(avg_repeat_p)\n",
    "        twenty_fifths_p.append(twenty_fifth_p)\n",
    "        medians_p.append(median_p)\n",
    "        seventy_fifths_p.append(seventy_fifth_p)\n",
    "        num_uniques_p.append(num_unique_p)  \n",
    "    \n",
    "results_df= pd.DataFrame({\n",
    "    \"Pareto\": pareto_alphas,\n",
    "    \"Capacity\": capacities,\n",
    "    'Mono_Pct': mono_pcts,\n",
    "    'Mono_Pct_P': mono_pcts_p,\n",
    "    \"Miscalibration\": miscals,\n",
    "    \"Hallucination\": hall_rates,\n",
    "    \"Log_Loss\": entropies,\n",
    "    \"Unseen_True_Rates\": unseen_true_rates,\n",
    "    \"Num_Training_Facts\": lengths_train,\n",
    "    \"Max_Train_Repeats\": max_train_repeats,\n",
    "    \"Avg_Train_Repeats\": average_train_repeats,\n",
    "    \"25_Train_Repeats\": twenty_fifths,\n",
    "    \"50_Train_Repeats\": medians,\n",
    "    \"70_Train_Repeats\": seventy_fifths,\n",
    "    \"Max_P_Repeats\": max_repeats_p,\n",
    "    \"Avg_P_Repeats\": average_repeats_p,\n",
    "    \"25_P_Repeats\": twenty_fifths_p,\n",
    "    \"50_P_Repeats\": medians_p,\n",
    "    \"70_P_Repeats\": seventy_fifths_p,\n",
    "    \"Num_Unique_Train\": num_uniques, \n",
    "    \"Num_Unique_P\": num_uniques_p\n",
    "    })\n",
    "\n",
    "results_df.to_csv(\"path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
