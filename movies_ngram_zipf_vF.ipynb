{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b5a921-592b-471c-82b6-26f2f818e537",
   "metadata": {},
   "source": [
    "## Create p distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7752799-1ff2-49f5-b200-44d1839a2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from scipy.stats import zipf\n",
    "import scipy.stats as stats\n",
    "random.seed(1217)\n",
    "F_dataset = pd.read_json('path', orient='records')\n",
    "##length is 417000\n",
    "def create_zipf_p(F_dataset, zipf_p):\n",
    "    #create p\n",
    "    all_facts = F_dataset['fact'].tolist()\n",
    "    new_facts = []\n",
    "    for fact in all_facts:\n",
    "        reps = zipf.rvs(a=zipf_p)\n",
    "        reps = int(np.ceil(reps))\n",
    "        new_facts.extend([fact] * reps)\n",
    "    ##calculate monofact rate\n",
    "    print(f'Monofact % in p is: {mono_calc(new_facts)}')\n",
    "    return new_facts\n",
    "\n",
    "def create_uniform_p(F_dataset):\n",
    "    #create p, which is already uniform\n",
    "    new_facts = F_dataset['fact'].tolist()\n",
    "    print(f'Monofact % is: {mono_calc(new_facts)}')\n",
    "    return new_facts\n",
    "\n",
    "def sample(new_facts, size):\n",
    "    #sample with replacement\n",
    "    training_data = random.sample(new_facts, k=size)\n",
    "    print(f'Monofact in % sample is: {mono_calc(training_data)}')\n",
    "    return training_data\n",
    "    #print out key stats\n",
    "\n",
    "def mono_calc(new_facts):\n",
    "    fact_counts = Counter(new_facts)\n",
    "    num_mono = sum(1 for count in fact_counts.values() if count == 1)\n",
    "    mono_pct = num_mono / len(new_facts)\n",
    "    return mono_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36c766-d408-4520-a979-af51a2eaecba",
   "metadata": {},
   "source": [
    "## Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7579b-fc13-4030-b08c-11af1c10b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieFactMarkovChain:\n",
    "    def __init__(self, order):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(lambda: defaultdict(float))\n",
    "        self.initial_counts = defaultdict(float)\n",
    "        self.starts = []\n",
    "        self.EOS = '<EOS>'\n",
    "        \n",
    "    def tokenize_fact(self, fact):\n",
    "        # Modify this method if city facts have a specific format\n",
    "        return fact.strip().split()\n",
    "    \n",
    "    def calculate_log_probability(self, fact):\n",
    "        \"\"\"Calculate log probability for a fact using order-n transitions.\"\"\"\n",
    "        tokens = self.tokenize_fact(fact) + [self.EOS]\n",
    "        if len(tokens) < self.order:\n",
    "            return np.log(1e-10)\n",
    "        \n",
    "        # Start with initial probability in log-space\n",
    "        start_tokens = tuple(tokens[:self.order])\n",
    "        log_probability = np.log(self.initial_probs.get(start_tokens, 1e-10))\n",
    "        \n",
    "        # Use order-n transitions\n",
    "        for i in range(len(tokens) - self.order):\n",
    "            current = tuple(tokens[i:i + self.order])\n",
    "            next_token = tokens[i + self.order]\n",
    "            \n",
    "            # Get transition probability\n",
    "            trans_prob = self.transitions.get(current, {}).get(next_token, 1e-10)\n",
    "            log_probability += np.log(trans_prob)\n",
    "        \n",
    "        return log_probability\n",
    "    \n",
    "    def train(self, facts):\n",
    "        print(f\"\\nTraining {self.order}-order Markov Chain...\")\n",
    "        num_facts = len(facts)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # First pass: collect transitions\n",
    "        print(\"Collecting transitions...\")\n",
    "        for fact in tqdm(facts, total=num_facts):\n",
    "            tokens = self.tokenize_fact(fact) + [self.EOS]\n",
    "            if len(tokens) < self.order + 1:\n",
    "                continue  # Skip if not enough tokens\n",
    "            \n",
    "            start_tokens = tuple(tokens[:self.order])\n",
    "            self.starts.append(start_tokens)\n",
    "            self.initial_counts[start_tokens] += 1\n",
    "            \n",
    "            for i in range(len(tokens) - self.order):\n",
    "                current = tuple(tokens[i:i + self.order])\n",
    "                next_token = tokens[i + self.order]\n",
    "                self.transitions[current][next_token] += 1\n",
    "        \n",
    "        # Second pass: normalize probabilities\n",
    "        print(\"Creating probabilities...\")\n",
    "        \n",
    "        total_starts = sum(self.initial_counts.values())\n",
    "        self.initial_probs = {start: count / total_starts for start, count in self.initial_counts.items()}\n",
    "        \n",
    "        for current, next_tokens in tqdm(self.transitions.items()):\n",
    "            total = sum(next_tokens.values())\n",
    "            for next_token in next_tokens:\n",
    "                next_tokens[next_token] /= total\n",
    "                    \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "        print(f\"Model trained on {num_facts:,} facts\")\n",
    "        print(f\"Unique state transitions: {len(self.transitions):,}\")\n",
    "    \n",
    "    def generate_facts(self, num_generations, output_path):\n",
    "        \"\"\"Generate new facts\"\"\"\n",
    "        generated_facts = []\n",
    "        print(f\"\\nGenerating {num_generations:,} facts...\")\n",
    "        \n",
    "        with tqdm(total=num_generations) as pbar:\n",
    "            while len(generated_facts) < num_generations:\n",
    "                # Pick a random start sequence\n",
    "                current = random.choices(\n",
    "                    population=list(self.initial_probs.keys()),\n",
    "                    weights=list(self.initial_probs.values()),\n",
    "                    k=1\n",
    "                )[0]\n",
    "                result = list(current)\n",
    "                \n",
    "                # Generate until EOS token is produced\n",
    "                while True:\n",
    "                    if current not in self.transitions:\n",
    "                        break  # Can't continue from here\n",
    "                    \n",
    "                    # Get possible next tokens and their probabilities\n",
    "                    possible_tokens = list(self.transitions[current].keys())\n",
    "                    probabilities = list(self.transitions[current].values())\n",
    "                    \n",
    "                    # Generate next token\n",
    "                    next_token = np.random.choice(possible_tokens, p=probabilities)\n",
    "                    if next_token == self.EOS:\n",
    "                        break  # End of sequence\n",
    "                    result.append(next_token)\n",
    "                    \n",
    "                    # Update current context\n",
    "                    current = tuple(result[-self.order:])\n",
    "                \n",
    "                # Add generated fact\n",
    "                generated_fact = ' '.join(result)\n",
    "                generated_facts.append({\n",
    "                    'fact': generated_fact,\n",
    "                    'generated': 1\n",
    "                })\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Create DataFrame and save\n",
    "            generated_df = pd.DataFrame(generated_facts)\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            generated_df.to_json(output_path, orient='records', lines=True)\n",
    "            \n",
    "            print(f\"\\nGenerated facts saved to: {output_path}\")\n",
    "            print(\"\\nSample generations:\")\n",
    "            print(generated_df['fact'].head().to_string(index=False))\n",
    "            \n",
    "            return generated_df\n",
    "\n",
    "#-----------------------------------------check hallucinations and calibration-----------------------------------------------\n",
    "def check_hallucinations(generated_data, training_data, F_dataset):\n",
    "    \"\"\"Analyze hallucination rates in generated data\n",
    "    \n",
    "    Hallucinations are generated facts that don't exist in F (false statements)\n",
    "    Not just facts missing from training data O\n",
    "    \"\"\"\n",
    "    print(\"\\nChecking hallucinations...\")\n",
    "    \n",
    "    generated_facts = set(generated_data['fact'])\n",
    "    F_facts = set(F_dataset)  # All true facts\n",
    "    \n",
    "    # Hallucinations are generated facts not in F\n",
    "    hallucinations = list(generated_facts - F_facts)\n",
    "    hallucination_rate = len(hallucinations) / len(generated_data)\n",
    "    \n",
    "    # Also calculate \"unseen but true\" rate\n",
    "    training_facts = set(training_data)\n",
    "    unseen_true = list(generated_facts - training_facts - set(hallucinations))\n",
    "    unseen_true_rate = len(unseen_true) / len(generated_data)\n",
    "    \n",
    "    print(f\"\\nHallucination rate (false facts): {hallucination_rate:.2%}\")\n",
    "    print(f\"Unseen but true facts rate: {unseen_true_rate:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'hallucination_rate': hallucination_rate,\n",
    "        'hallucinated_facts': hallucinations,\n",
    "        'unseen_true_rate': unseen_true_rate,\n",
    "        'unseen_true_facts': unseen_true\n",
    "    }\n",
    "\n",
    "def create_epsilon_induced_bins(epsilon: float):\n",
    "   \"\"\"\n",
    "   Creates bins according to Definition 7 in the paper (page 169).\n",
    "   \n",
    "   Args:\n",
    "       epsilon: Controls the bin width (between 0 and 1)\n",
    "       \n",
    "   Returns:\n",
    "       List of tuples representing (bin_start, bin_end) in probability space\n",
    "   \"\"\"\n",
    "   if epsilon < 0:\n",
    "       raise ValueError(\"Epsilon must be non-negative\")\n",
    "   \n",
    "   if epsilon == 0:\n",
    "       # For epsilon = 0, we create a bin for each unique probability in g\n",
    "       # This will be handled in the main calibration function\n",
    "       return \"finest\"\n",
    "   \n",
    "   if epsilon >= 1:\n",
    "       # When epsilon = 1, return single bin for all probabilities\n",
    "       return [(0, 1)]\n",
    "   \n",
    "   bins = []\n",
    "   i = 0\n",
    "   \n",
    "   while True:\n",
    "       upper = (1-epsilon)**i\n",
    "       lower = (1-epsilon)**(i+1)\n",
    "       \n",
    "       # If lower bound gets very small, make it 0 and make this our last bin\n",
    "       if lower < 1e-10:\n",
    "           bins.append((0, upper))\n",
    "           break\n",
    "           \n",
    "       if upper - lower > 1e-10:\n",
    "           bins.append((lower, upper))\n",
    "       \n",
    "       i += 1\n",
    "   ##append final bin for edge case where everything is 1\n",
    "   bins.append((1.0, 1.0))\n",
    "    \n",
    "   return bins\n",
    "    \n",
    "def calculate_kalai_calibration_log_binning(F_dataset, model, epsilon, output_path):\n",
    "    \n",
    "    # Define training facts from O_dataset and get frequencies\n",
    "    all_facts = F_dataset\n",
    "    fact_counts = Counter(all_facts)\n",
    "    total_facts = len(all_facts)\n",
    "    \n",
    "    facts_list = []\n",
    "    g_prob_list = []\n",
    "    p_prob_list = []\n",
    "    \n",
    "    raw_g_prob_list = []\n",
    "    raw_p_prob_list = []\n",
    "    \n",
    "    for fact in tqdm(all_facts):\n",
    "        log_prob = model.calculate_log_probability(fact)\n",
    "        g_prob = np.exp(log_prob)\n",
    "\n",
    "        facts_list.append(fact)\n",
    "        g_prob_list.append(g_prob)\n",
    "        raw_g_prob_list.append(g_prob)\n",
    "        \n",
    "        p_prob = fact_counts[fact] / total_facts\n",
    "        p_prob_list.append(p_prob)\n",
    "        raw_p_prob_list.append(p_prob)\n",
    "\n",
    "    # Normalize g(y) and p(y)\n",
    "    g_sum = sum(g_prob_list)\n",
    "    if g_sum > 0:\n",
    "        g_prob_list = [p / g_sum for p in g_prob_list]\n",
    "    p_sum = sum(p_prob_list)\n",
    "    if p_sum > 0:\n",
    "        p_prob_list = [p / p_sum for p in p_prob_list]\n",
    "\n",
    "    # Create bins\n",
    "    bins = create_epsilon_induced_bins(epsilon)\n",
    "    if isinstance(bins, str) and bins == \"finest\":\n",
    "        unique_probs = sorted(set(g_prob_list), reverse=True)\n",
    "        bins = [(p, p) for p in unique_probs]\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # 1) Create data structures to accumulate bin memberships + sums.\n",
    "    # ----------------------------------------------------------------\n",
    "    binned_facts = [[] for _ in range(len(bins))]\n",
    "\n",
    "    # Keep track of the sum of p(y) and g(y) in each bin\n",
    "    binned_p_sums = [0.0] * len(bins)\n",
    "    binned_g_sums = [0.0] * len(bins)\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # 2) Assign each fact to exactly one bin and update running sums.\n",
    "    # -------------------------------------------------------------\n",
    "    for i, (fact, g_y) in enumerate(zip(facts_list, g_prob_list)):\n",
    "        assigned = False\n",
    "        for bin_idx, (lower, upper) in enumerate(bins):\n",
    "            # If it falls into [lower, upper)\n",
    "            if lower <= g_y < upper:\n",
    "                binned_facts[bin_idx].append((fact, g_y))\n",
    "                binned_p_sums[bin_idx] += p_prob_list[i]\n",
    "                binned_g_sums[bin_idx] += g_prob_list[i]\n",
    "                assigned = True\n",
    "                break\n",
    "        # If we never broke out, it goes to the last bin\n",
    "        # (Handles edge-case if upper=1 for last bin.)\n",
    "        if not assigned:\n",
    "            last_idx = len(bins) - 1\n",
    "            binned_facts[last_idx].append((fact, g_y))\n",
    "            binned_p_sums[last_idx] += p_prob_list[i]\n",
    "            binned_g_sums[last_idx] += g_prob_list[i]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # 3) Calculate miscalibration efficiently\n",
    "    # ---------------------------------------\n",
    "    miscalibration = 0.0\n",
    "    for bin_idx in range(len(bins)):\n",
    "        p_B = binned_p_sums[bin_idx]\n",
    "        g_B = binned_g_sums[bin_idx]\n",
    "        miscalibration += abs(p_B - g_B)\n",
    "    \n",
    "    miscalibration *= 0.5\n",
    "\n",
    "    entropy = 0\n",
    "    for prob in g_prob_list:\n",
    "        if prob > 0:\n",
    "            entropy -= prob * np.log2(prob)\n",
    "    print(f\"\\nFinal entropy: {entropy:.6e}\")\n",
    "    \n",
    "    return {\n",
    "        'miscalibration': float(miscalibration),\n",
    "        'num_bins': len(binned_facts),\n",
    "        'entropy': float(entropy),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcd449-6a4e-4eb3-ae65-96a1aba3ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "uniform_p = pd.read_json('path', orient='records')\n",
    "uniform_p = uniform_p[1:20000]\n",
    "print(uniform_p)\n",
    "set_order = 2\n",
    "generation_size = 5000\n",
    "sample_size = 5000\n",
    "bin_num = 25\n",
    "epsilon = 0.1\n",
    "\n",
    "hall_rates = []\n",
    "unseen_true_rates = []\n",
    "zipf_ps = []\n",
    "mono_pcts = []\n",
    "miscals = []\n",
    "lengths_train = []\n",
    "max_train_repeats  = []\n",
    "average_train_repeats = []\n",
    "entropies = []\n",
    "\n",
    "zipf_p_list = np.arange(1.8, 6, 0.01).tolist()\n",
    "for zipf_p in zipf_p_list:\n",
    "    print(f\"\\nProcessing zipf's p: {zipf_p}\")\n",
    "    # p distribution\n",
    "    zipf_p_distr = create_zipf_p(uniform_p, zipf_p)\n",
    "    training_data = sample(zipf_p_distr, sample_size)\n",
    "\n",
    "    # check average and max repeats in training data\n",
    "    facts_counter = Counter(training_data)\n",
    "    max_repeats = max(facts_counter.values())\n",
    "    avg_repeats = sum(facts_counter.values()) / len(facts_counter)\n",
    "    \n",
    "    # mono rate calc\n",
    "    mono_rate = mono_calc(training_data)\n",
    "    \n",
    "    # fit model\n",
    "    model = MovieFactMarkovChain(order=set_order)\n",
    "    model.train(training_data)\n",
    "    \n",
    "    #generation path for fact checking\n",
    "    gen_path = f\"path\"\n",
    "    os.makedirs(os.path.dirname(gen_path), exist_ok=True)\n",
    "    generated_data = model.generate_facts(generation_size, gen_path)\n",
    "    #hallucination results\n",
    "    hall_results = check_hallucinations(generated_data, training_data, zipf_p_distr)\n",
    "\n",
    "    #miscalibration results\n",
    "    miscal_path = f\"path\"\n",
    "    miscal_results = calculate_kalai_calibration_log_binning(zipf_p_distr, model, epsilon, miscal_path)\n",
    "    \n",
    "    #save results\n",
    "    mono_pcts.append(mono_rate)\n",
    "    hall_rates.append(hall_results['hallucination_rate'])\n",
    "    unseen_true_rates.append(hall_results['unseen_true_rate'])\n",
    "    miscals.append(miscal_results['miscalibration'])\n",
    "    entropies.append(miscal_results['entropy'])\n",
    "    lengths_train.append(len(training_data))\n",
    "    max_train_repeats.append(max_repeats)\n",
    "    average_train_repeats.append(avg_repeats)\n",
    "    zipf_ps.append(zipf_p)\n",
    "    \n",
    "results_df= pd.DataFrame({\n",
    "    'Mono_Pct': mono_pcts,\n",
    "    \"Hallucination\": hall_rates,\n",
    "    \"Zipf_Parameter\": zipf_ps,\n",
    "    \"Unseen_True_Rates\": unseen_true_rates,\n",
    "    \"Miscalibration\": miscals,\n",
    "    \"Entropy\": entropies,\n",
    "    \"Num_Training_Facts\": lengths_train,\n",
    "    \"Max_Train_Repeats\": max_train_repeats,\n",
    "    \"Avg_Train_Repeats\": average_train_repeats\n",
    "    })\n",
    "\n",
    "results_df['Mono - Miscal'] = results_df[\"Mono_Pct\"] - results_df[\"Miscalibration\"]\n",
    "results_df.to_csv(\"path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
